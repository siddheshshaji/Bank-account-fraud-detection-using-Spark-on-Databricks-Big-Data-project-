{"cells":[{"cell_type":"markdown","source":["# Load data to Delta Lake from S3 with COPY INTO\n\nThis notebook shows you how to create a Delta Lake from by using `COPY INTO` to load data from AWS S3.\n\nThe examples below use CSV as a file source; for more details and additional options, see [Load data with COPY INTO](https://docs.databricks.com/ingestion/copy-into/index.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0c6f131-9b7b-4193-8b42-dd6ddcd9e478","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Create a target Delta table\n\n`COPY INTO` requires a target table created with Delta Lake. If using Databricks Runtime (DBR) 11.0 or above, you can create an empty Delta table using the command below.\n\nWhen using DBR below 11.0, you'll need to specify the schema for the table during creation.\n\nDelta Lake is the default format for all tables created in DBR 8.0 and above. When using DBR below 8.0, you'll need to add a `USING DELTA` clause to your create table statement."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af34de32-5497-4823-ab4e-2bd4a03cdda9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\n-- 11.0 and above\nCREATE TABLE <database-name>.<table-name>;\n\n-- 8.0 and above\n-- CREATE TABLE <database-name>.<table-name>\n-- (col_1 TYPE, col_2 TYPE, ...);\n\n-- Below 8.0\n-- CREATE TABLE <database-name>.<table-name>\n-- (col_1 TYPE, col_2 TYPE, ...)\n-- USING delta;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8c1d984-2a44-42bb-995a-5eba97214e7b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Loading data with an instance profile\n\nUsers with sufficient permissions can create instance profiles in the AWS console.\n\nDatabricks administrator can load instance profiles for use in the Databricks workspace.\n\nDatabricks recommends securing access to S3 buckets by attaching instance profiles to clusters.\n\n* [Databricks docs: Secure access to S3 buckets using instance profiles](https://docs.databricks.com/administration-guide/cloud-configurations/aws/instance-profiles.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d43a03c-eb1d-4a86-a3e9-8956b0cea8d2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nCOPY INTO <database-name>.<table-name>\nFROM 's3://bucket-name/path/to/folder'\nFILEFORMAT = CSV\nCOPY_OPTIONS ('mergeSchema' = 'true')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b9cfac3-662d-4c21-924a-270e36f1e8c4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## COPY INTO with temporary credentials\n\n`COPY INTO` also supports using temporary credentials to access data from S3 buckets.\n\n* [Databricks docs: Use temporary credentials to load data with COPY INTO](https://docs.databricks.com/ingestion/copy-into/temporary-credentials.html)\n\nYou can use the AWS CLI to generate the access key, secret key, and session token you'll need to access the S3 bucket. Note that this process merely provides authentication to AWS, and will just allow Databricks to access your S3 bucket with your user credentials. (If you do not have permissions to access the S3 bucket in AWS, you will need to talk to your cloud administrator.)\n\n* [AWS docs: Developing with Amazon S3 using the AWS CLI](https://docs.aws.amazon.com/AmazonS3/latest/userguide/setup-aws-cli.html)\n* [AWS docs: AWS CLI sts get-session-token](https://docs.aws.amazon.com/cli/latest/reference/sts/get-session-token.html)\n\nFor more details on IAM user temporary credentials and a complete set of programmatic options, see:\n* [AWS docs: Making requests using IAM user temporary credentials](https://docs.aws.amazon.com/AmazonS3/latest/userguide/AuthUsingTempSessionToken.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4fa05c54-a8be-4ee2-b300-e121f1f21fa8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nCOPY INTO <database-name>.<table-name>\nFROM 's3://bucket-name/path/to/folder' WITH (\n  CREDENTIAL (AWS_ACCESS_KEY = '<access-key>', AWS_SECRET_KEY = '<secret-key>', AWS_SESSION_TOKEN = '<session-token>')\n)\nFILEFORMAT = CSV\nCOPY_OPTIONS ('mergeSchema' = 'true')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"caeed824-8c68-4237-90be-c60142b95a4f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Amazon S3","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3947743459094892}},"nbformat":4,"nbformat_minor":0}
